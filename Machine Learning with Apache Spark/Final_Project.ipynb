{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtAVuFbeK8WH"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark==3.1.2 -q\n",
        "!pip install findspark -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M92KYZDaK8WI"
      },
      "outputs": [],
      "source": [
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# FindSpark simplifies the process of using Apache Spark with Python\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP1OZMQ-K8WJ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.pipeline import PipelineModel\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el8zK0HdK8WK",
        "outputId": "bd383859-20a5-4d05-f162-4b41fbd0609c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/06/11 22:40:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        }
      ],
      "source": [
        "#Create SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Practice Project\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOvYFzNHK8WK",
        "outputId": "e8b7fdc3-8991-4c34-eb83-58299db70acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-11 22:40:31--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg-raw.csv\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14354 (14K) [text/csv]\n",
            "Saving to: ‘mpg-raw.csv.1’\n",
            "\n",
            "mpg-raw.csv.1       100%[===================>]  14.02K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-06-11 22:40:31 (17.9 MB/s) - ‘mpg-raw.csv.1’ saved [14354/14354]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg-raw.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dabV_-QK8WL"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "df = spark.read.csv(\"mpg-raw.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7P2FaGYK8WM"
      },
      "source": [
        "### Task 4 - Print top 5 rows of the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1juqQVYK8WM",
        "outputId": "0c4cb87e-fedd-4424-a560-0b6c430f5a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---------+-----------+----------+------+----------+----+--------+\n",
            "| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|\n",
            "+----+---------+-----------+----------+------+----------+----+--------+\n",
            "|46.6|        4|       86.0|        65|  2110|      17.9|  80|Japanese|\n",
            "|44.6|        4|       91.0|        67|  1850|      13.8|  80|Japanese|\n",
            "|44.3|        4|       90.0|        48|  2085|      21.7|  80|European|\n",
            "|44.0|        4|       97.0|        52|  2130|      24.6|  82|European|\n",
            "|43.4|        4|       90.0|        48|  2335|      23.7|  80|European|\n",
            "+----+---------+-----------+----------+------+----------+----+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpnhZj0hK8WN",
        "outputId": "19fd58fd-d234-4a55-9233-c7ca6248aafa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|  Origin|count|\n",
            "+--------+-----+\n",
            "|    null|    1|\n",
            "|European|   70|\n",
            "|Japanese|   88|\n",
            "|American|  247|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy('Origin').count().orderBy('count').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcB0msjQK8WN",
        "outputId": "244bcef5-ca0b-4cb8-d427-14ab0a2eaa4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "406\n"
          ]
        }
      ],
      "source": [
        "rowcount1 = df.count()\n",
        "print(rowcount1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ciq4gPvUK8WO"
      },
      "outputs": [],
      "source": [
        "df = df.dropDuplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfYBUHIAK8WP",
        "outputId": "72a3fd9f-fb28-4117-91e9-5322103c0d3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 8:===================================================>   (186 + 9) / 200]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "rowcount2 = df.count()\n",
        "print(rowcount2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMqQJ6VCK8WP"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWOXGWCqK8WQ",
        "outputId": "7e04d33d-6725-49ed-fba2-d0abc6d1ac83"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 11:==================================================>   (186 + 8) / 200]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "385\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "rowcount3 = df.count()\n",
        "print(rowcount3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdFJbNRFK8WQ"
      },
      "outputs": [],
      "source": [
        "df = df.withColumnRenamed(\"Engine Disp\",\"Engine_Disp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbLbxpGQK8WR",
        "outputId": "cee4f658-2c2b-43d6-bf71-752336cfdc60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 14:=======>                                               (28 + 8) / 200]25/06/11 22:41:22 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
            "Scaling row group sizes to 96.54% for 7 writers\n",
            "25/06/11 22:41:22 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
            "Scaling row group sizes to 84.47% for 8 writers\n",
            "25/06/11 22:41:22 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
            "Scaling row group sizes to 96.54% for 7 writers\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df.write.mode(\"overwrite\").parquet(\"mpg-cleaned.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmQoMayvK8WS",
        "outputId": "bd1559c2-1908-432d-b1c4-6a6b8d866251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Part 1 - Evaluation\n",
            "Total rows =  406\n",
            "Total rows after dropping duplicate rows =  392\n",
            "Total rows after dropping duplicate rows and rows with null values =  385\n",
            "Renamed column name =  Engine_Disp\n",
            "mpg-cleaned.parquet exists : True\n"
          ]
        }
      ],
      "source": [
        "print(\"Part 1 - Evaluation\")\n",
        "\n",
        "print(\"Total rows = \", rowcount1)\n",
        "print(\"Total rows after dropping duplicate rows = \", rowcount2)\n",
        "print(\"Total rows after dropping duplicate rows and rows with null values = \", rowcount3)\n",
        "print(\"Renamed column name = \", df.columns[2])\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"mpg-cleaned.parquet exists :\", os.path.isdir(\"mpg-cleaned.parquet\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92AArcO-K8WS",
        "outputId": "f9017592-2097-444e-c0fd-942e1daba1dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df = spark.read.parquet(\"mpg-cleaned.parquet\")\n",
        "rowcount4 = df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itZA-1SVK8WS",
        "outputId": "b4725988-050b-421a-b5bd-e4dbe1af549e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---------+-----------+----------+------+----------+----+--------+\n",
            "| MPG|Cylinders|Engine_Disp|Horsepower|Weight|Accelerate|Year|  Origin|\n",
            "+----+---------+-----------+----------+------+----------+----+--------+\n",
            "|32.2|        4|      108.0|        75|  2265|      15.2|  80|Japanese|\n",
            "|28.0|        4|      107.0|        86|  2464|      15.5|  76|European|\n",
            "|26.0|        4|      156.0|        92|  2585|      14.5|  82|American|\n",
            "|25.0|        4|      104.0|        95|  2375|      17.5|  70|European|\n",
            "|25.0|        4|      140.0|        75|  2542|      17.0|  74|American|\n",
            "+----+---------+-----------+----------+------+----------+----+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- MPG: double (nullable = true)\n",
            " |-- Cylinders: integer (nullable = true)\n",
            " |-- Engine_Disp: double (nullable = true)\n",
            " |-- Horsepower: integer (nullable = true)\n",
            " |-- Weight: integer (nullable = true)\n",
            " |-- Accelerate: double (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Origin: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(5)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV7xcuq5K8WT"
      },
      "outputs": [],
      "source": [
        "# Stage - 1 Using StringIndexer convert the string column \"Origin\" into \"OriginIndex\"\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"Origin\", outputCol=\"OriginIndex\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSmtn9gfK8WT"
      },
      "outputs": [],
      "source": [
        "# Stage 2 - assemble the input columns 'Cylinders','Engine_Disp','Horsepower','Weight','Accelerate','Year' into a single column \"features\"\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=['Cylinders', 'Engine_Disp', 'Horsepower', 'Weight', 'Accelerate', 'Year'],\n",
        "    outputCol=\"features\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icX-ZASyK8WU"
      },
      "outputs": [],
      "source": [
        "# Stage 3 - scale the \"features\" using standard scaler and store in \"scaledFeatures\" column\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXvHHPxKK8WV"
      },
      "outputs": [],
      "source": [
        "# Stage 4 - Create a LinearRegression stage to predict \"MPG\"\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"MPG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_PYhfHEK8WX"
      },
      "outputs": [],
      "source": [
        "# Build a pipeline using the above four stages\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, assembler, scaler, lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnIE1m3BK8WX"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets with 70:30 split. Use 42 as seed\n",
        "(trainingData, testingData) = df.randomSplit([0.7, 0.3], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0pYrak6K8WY",
        "outputId": "8fd6c2fc-ed5f-4fb2-9fa0-7a57ba133df7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/06/11 22:47:00 WARN util.Instrumentation: [5e8d2f50] regParam is zero, which might cause numerical instability and overfitting.\n",
            "[Stage 24:>                                                         (0 + 8) / 8]25/06/11 22:47:02 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
            "25/06/11 22:47:02 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
            "25/06/11 22:47:02 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
            "25/06/11 22:47:02 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Fit the pipeline using the training data\n",
        "\n",
        "pipelineModel = pipeline.fit(trainingData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR23m-foK8WZ",
        "outputId": "e121e6d3-cf28-430f-eec4-e2931010202e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Part 2 - Evaluation\n",
            "Total rows =  385\n",
            "Pipeline Stage 1 =  StringIndexer\n",
            "Pipeline Stage 2 =  VectorAssembler\n",
            "Pipeline Stage 3 =  StandardScaler\n",
            "Label column =  MPG\n"
          ]
        }
      ],
      "source": [
        "print(\"Part 2 - Evaluation\")\n",
        "print(\"Total rows = \", rowcount4)\n",
        "ps = [str(x).split(\"_\")[0] for x in pipeline.getStages()]\n",
        "\n",
        "print(\"Pipeline Stage 1 = \", ps[0])\n",
        "print(\"Pipeline Stage 2 = \", ps[1])\n",
        "print(\"Pipeline Stage 3 = \", ps[2])\n",
        "\n",
        "print(\"Label column = \", lr.getLabelCol())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY-UbVNlK8WZ"
      },
      "outputs": [],
      "source": [
        "# Make predictions on testing data\n",
        "predictions = pipelineModel.transform(testingData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKUjOib9K8Wa",
        "outputId": "0288ed66-1303-49de-b8af-4d8b61124287"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 31:>                                                         (0 + 8) / 8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.899168410636614\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"MPG\", metricName=\"mse\")\n",
        "mse = evaluator.evaluate(predictions)\n",
        "print(mse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDioOCaAK8Wa",
        "outputId": "6188b541-5116-40d4-ab7e-1c154e88154c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 33:>                                                         (0 + 8) / 8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.625684006095087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"MPG\", metricName=\"mae\")\n",
        "mae = evaluator.evaluate(predictions)\n",
        "print(mae)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogInPhrEK8Wb"
      },
      "source": [
        "### Task 4 - Print the R-Squared(R2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1pQymq9K8Wb",
        "outputId": "cd1770c4-66ef-4ad0-c30c-a89a12c0e744"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 35:>                                                         (0 + 8) / 8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8202762026606879\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"MPG\", metricName=\"r2\")\n",
        "r2 = evaluator.evaluate(predictions)\n",
        "print(r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqcVm5-cK8Wc",
        "outputId": "11dcc0a8-534c-4075-a5f9-f8f4c1ea9cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Part 3 - Evaluation\n",
            "Mean Squared Error =  10.9\n",
            "Mean Absolute Error =  2.63\n",
            "R Squared =  0.82\n",
            "Intercept =  -13.9\n"
          ]
        }
      ],
      "source": [
        "print(\"Part 3 - Evaluation\")\n",
        "\n",
        "print(\"Mean Squared Error = \", round(mse,2))\n",
        "print(\"Mean Absolute Error = \", round(mae,2))\n",
        "print(\"R Squared = \", round(r2,2))\n",
        "\n",
        "lrModel = pipelineModel.stages[-1]\n",
        "\n",
        "print(\"Intercept = \", round(lrModel.intercept,2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M4kg4x3K8Wc",
        "outputId": "045633ba-2404-4c38-b5ed-7df415501684"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Save the pipeline model\n",
        "# your code goes here\n",
        "pipelineModel.write().save(\"Practice_Project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3vAHX3TK8Wd"
      },
      "outputs": [],
      "source": [
        "# Load the pipeline model\n",
        "# your code goes here\n",
        "loadedPipelineModel = PipelineModel.load(\"Practice_Project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZMJp3X5K8We"
      },
      "outputs": [],
      "source": [
        "# Use the loaded pipeline model for predictions\n",
        "# your code goes here\n",
        "predictions = loadedPipelineModel.transform(testingData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI8YswPRK8We",
        "outputId": "eb4b47de-ec7b-4394-d206-7423e89ec1c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 80:>                                                         (0 + 1) / 1]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------------------+\n",
            "| MPG|        prediction|\n",
            "+----+------------------+\n",
            "|13.0| 11.31880991495555|\n",
            "|13.0|14.368910120039454|\n",
            "|13.0|10.684980370654229|\n",
            "|15.0| 13.06659233990255|\n",
            "|15.5|15.669787158219199|\n",
            "|18.0| 19.81597780531922|\n",
            "|18.0|22.299804385989376|\n",
            "|18.0|20.053788782604926|\n",
            "|18.6|20.890856761279736|\n",
            "|19.0|24.862180440385856|\n",
            "|21.5|26.265038242693542|\n",
            "|22.0| 23.09860601340658|\n",
            "|23.0| 21.27831362520896|\n",
            "|24.0|22.969583765345746|\n",
            "|25.1|27.030283144018128|\n",
            "|26.0|27.950969788034072|\n",
            "|26.0|25.754412517142313|\n",
            "|27.0| 28.40010908515616|\n",
            "|29.0|28.123745554548947|\n",
            "|30.0| 30.24946672093104|\n",
            "+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "predictions.select(\"MPG\",\"prediction\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oan3P8nEK8Wf"
      },
      "outputs": [],
      "source": [
        "print(\"Part 4 - Evaluation\")\n",
        "\n",
        "loadedmodel = loadedPipelineModel.stages[-1]\n",
        "totalstages = len(loadedPipelineModel.stages)\n",
        "inputcolumns = loadedPipelineModel.stages[1].getInputCols()\n",
        "\n",
        "print(\"Number of stages in the pipeline = \", totalstages)\n",
        "for i,j in zip(inputcolumns, loadedmodel.coefficients):\n",
        "    print(f\"Coefficient for {i} is {round(j,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G6aWCVCK8Wg"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nckESplKK8Wh"
      },
      "source": [
        "<!--\n",
        "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
        "|-|-|-|-|\n",
        "|2023-05-26|0.1|Ramesh Sannareddy|Initial Version Created|\n",
        "-->\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "prev_pub_hash": "38afc29663845f2324e5b73e44a5bc91f9b60268d7dee3253e149c7c1bfdef31",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}