{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58bHxvMnEJhS"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark==3.1.2 -q\n",
        "!pip install findspark -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x7ciNg6EJhT"
      },
      "outputs": [],
      "source": [
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# FindSpark simplifies the process of using Apache Spark with Python\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import rand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu44owOyEJhT",
        "outputId": "6df1b6ad-a386-49f1-b6dd-90add4f0d08e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/07/08 21:55:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        }
      ],
      "source": [
        "#Create SparkSession\n",
        "#Ignore any warnings by SparkSession command\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Feature Extraction and Transformation using Spark\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHdHcPBOEJhU"
      },
      "outputs": [],
      "source": [
        "#import tokenizer\n",
        "from pyspark.ml.feature import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG03vnHgEJhU"
      },
      "outputs": [],
      "source": [
        "#create a sample dataframe\n",
        "sentenceDataFrame = spark.createDataFrame([\n",
        "    (1, \"Spark is a distributed computing system.\"),\n",
        "    (2, \"It provides interfaces for multiple languages\"),\n",
        "    (3, \"Spark is built on top of Hadoop\")\n",
        "], [\"id\", \"sentence\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y13G3Cc1EJhU",
        "outputId": "038f1a18-0e01-4138-d9f3-85415e227cf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------------------------------------------+\n",
            "|id |sentence                                     |\n",
            "+---+---------------------------------------------+\n",
            "|1  |Spark is a distributed computing system.     |\n",
            "|2  |It provides interfaces for multiple languages|\n",
            "|3  |Spark is built on top of Hadoop              |\n",
            "+---+---------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#display the dataframe\n",
        "sentenceDataFrame.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktwxGk-0EJhU"
      },
      "outputs": [],
      "source": [
        "#create tokenizer instance.\n",
        "#mention the column to be tokenized as inputcol\n",
        "#mention the output column name where the tokens are to be stored.\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTUXBUfWEJhV"
      },
      "outputs": [],
      "source": [
        "#tokenize\n",
        "token_df = tokenizer.transform(sentenceDataFrame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSCT7T-tEJhV",
        "outputId": "3189e6ea-1c37-48d2-aeba-6dac38840923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------------------------------------------+----------------------------------------------------+\n",
            "|id |sentence                                     |words                                               |\n",
            "+---+---------------------------------------------+----------------------------------------------------+\n",
            "|1  |Spark is a distributed computing system.     |[spark, is, a, distributed, computing, system.]     |\n",
            "|2  |It provides interfaces for multiple languages|[it, provides, interfaces, for, multiple, languages]|\n",
            "|3  |Spark is built on top of Hadoop              |[spark, is, built, on, top, of, hadoop]             |\n",
            "+---+---------------------------------------------+----------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#display the tokenized data\n",
        "token_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GgM7OinEJhV"
      },
      "outputs": [],
      "source": [
        "#import CountVectorizer\n",
        "from pyspark.ml.feature import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANsYDaCREJhW",
        "outputId": "81220759-b304-47e7-cbd8-9d2a736ce8ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------------------------------------------------+\n",
            "|id |words                                            |\n",
            "+---+-------------------------------------------------+\n",
            "|1  |[I, love, Spark, Spark, provides, Python, API]   |\n",
            "|2  |[I, love, Python, Spark, supports, Python]       |\n",
            "|3  |[Spark, solves, the, big, problem, of, big, data]|\n",
            "+---+-------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#create a sample dataframe and display it.\n",
        "textdata = [(1, \"I love Spark Spark provides Python API \".split()),\n",
        "            (2, \"I love Python Spark supports Python\".split()),\n",
        "            (3, \"Spark solves the big problem of big data\".split())]\n",
        "\n",
        "textdata = spark.createDataFrame(textdata, [\"id\", \"words\"])\n",
        "\n",
        "textdata.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFTGaJ17EJhX"
      },
      "outputs": [],
      "source": [
        "# Create a CountVectorizer object\n",
        "# mention the column to be count vectorized as inputcol\n",
        "# mention the output column name where the count vectors are to be stored.\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcmE2hvMEJhX",
        "outputId": "a69ea6bc-1385-41fe-ffdd-79c4b64ed5ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Fit the CountVectorizer model on the input data\n",
        "model = cv.fit(textdata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSsFOrRhEJhX"
      },
      "outputs": [],
      "source": [
        "# Transform the input data to bag-of-words vectors\n",
        "result = model.transform(textdata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9VTBb8nEJhX",
        "outputId": "8aec2cf8-c90d-4d13-bede-9471a71eeda0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------------------------------------------------+---------------------------------------------------+\n",
            "|id |words                                            |features                                           |\n",
            "+---+-------------------------------------------------+---------------------------------------------------+\n",
            "|1  |[I, love, Spark, Spark, provides, Python, API]   |(13,[0,1,2,4,10,11],[2.0,1.0,1.0,1.0,1.0,1.0])     |\n",
            "|2  |[I, love, Python, Spark, supports, Python]       |(13,[0,1,2,4,9],[1.0,2.0,1.0,1.0,1.0])             |\n",
            "|3  |[Spark, solves, the, big, problem, of, big, data]|(13,[0,3,5,6,7,8,12],[1.0,2.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "+---+-------------------------------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# display the dataframe\n",
        "result.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm9huvxVEJhY"
      },
      "outputs": [],
      "source": [
        "#import necessary classes for TF-IDF calculation\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRKLIKlzEJhY",
        "outputId": "93f9ca90-1f81-4791-e03b-d6e849b52f41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------------------+\n",
            "|id |sentence             |\n",
            "+---+---------------------+\n",
            "|1  |Spark supports python|\n",
            "|2  |Spark is fast        |\n",
            "|3  |Spark is easy        |\n",
            "+---+---------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#create a sample dataframe and display it.\n",
        "sentenceData = spark.createDataFrame([\n",
        "        (1, \"Spark supports python\"),\n",
        "        (2, \"Spark is fast\"),\n",
        "        (3, \"Spark is easy\")\n",
        "    ], [\"id\", \"sentence\"])\n",
        "\n",
        "sentenceData.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuj9nc12EJhY",
        "outputId": "2d02fa37-307e-438c-bac6-cabf8d9367bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------------------+-------------------------+\n",
            "|id |sentence             |words                    |\n",
            "+---+---------------------+-------------------------+\n",
            "|1  |Spark supports python|[spark, supports, python]|\n",
            "|2  |Spark is fast        |[spark, is, fast]        |\n",
            "|3  |Spark is easy        |[spark, is, easy]        |\n",
            "+---+---------------------+-------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#tokenize the \"sentence\" column and store in the column \"words\"\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "wordsData.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVOcuCnhEJhZ",
        "outputId": "0ff4433b-c663-46ff-94ce-b913374aa6a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------------------+-------------------------+--------------------------+\n",
            "|id |sentence             |words                    |rawFeatures               |\n",
            "+---+---------------------+-------------------------+--------------------------+\n",
            "|1  |Spark supports python|[spark, supports, python]|(10,[4,5,9],[1.0,1.0,1.0])|\n",
            "|2  |Spark is fast        |[spark, is, fast]        |(10,[1,3,5],[1.0,1.0,1.0])|\n",
            "|3  |Spark is easy        |[spark, is, easy]        |(10,[0,1,5],[1.0,1.0,1.0])|\n",
            "+---+---------------------+-------------------------+--------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a HashingTF object\n",
        "# mention the \"words\" column as input\n",
        "# mention the \"rawFeatures\" column as output\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "featurizedData.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03sAOpyJEJhZ",
        "outputId": "d9fb101a-a75b-47cc-bdd3-c12cdd829d5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Create an IDF object\n",
        "# mention the \"rawFeatures\" column as input\n",
        "# mention the \"features\" column as output\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "tfidfData = idfModel.transform(featurizedData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X49ijETKEJhZ",
        "outputId": "f26bb058-a75f-4bd6-c310-388c2791ea5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------+---------------------------------------------------------+\n",
            "|sentence             |features                                                 |\n",
            "+---------------------+---------------------------------------------------------+\n",
            "|Spark supports python|(10,[4,5,9],[0.6931471805599453,0.0,0.6931471805599453]) |\n",
            "|Spark is fast        |(10,[1,3,5],[0.28768207245178085,0.6931471805599453,0.0])|\n",
            "|Spark is easy        |(10,[0,1,5],[0.6931471805599453,0.28768207245178085,0.0])|\n",
            "+---------------------+---------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#display the tf-idf data\n",
        "tfidfData.select(\"sentence\", \"features\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_bE-N-DEJha"
      },
      "outputs": [],
      "source": [
        "#import StopWordsRemover\n",
        "from pyspark.ml.feature import StopWordsRemover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF9wFrpdEJha",
        "outputId": "651e2fa9-f1cc-458f-8f0b-7942f4d7f3d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------------------------------------------------------------+\n",
            "|id |sentence                                                    |\n",
            "+---+------------------------------------------------------------+\n",
            "|1  |[Spark, is, an, open-source, distributed, computing, system]|\n",
            "|2  |[IT, has, interfaces, for, multiple, languages]             |\n",
            "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |\n",
            "+---+------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#create a dataframe with sample text and display it\n",
        "textData = spark.createDataFrame([\n",
        "    (1, ['Spark', 'is', 'an', 'open-source', 'distributed', 'computing', 'system']),\n",
        "    (2, ['IT', 'has', 'interfaces', 'for', 'multiple', 'languages']),\n",
        "    (3, ['It', 'has', 'a', 'wide', 'range', 'of', 'libraries', 'and', 'APIs'])\n",
        "], [\"id\", \"sentence\"])\n",
        "\n",
        "textData.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrcdcE13EJha"
      },
      "outputs": [],
      "source": [
        "# remove stopwords from \"sentence\" column and store the result in \"filtered_sentence\" column\n",
        "remover = StopWordsRemover(inputCol=\"sentence\", outputCol=\"filtered_sentence\")\n",
        "textData = remover.transform(textData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz-E6LypEJhb",
        "outputId": "65f16380-4334-4240-f02f-b96ce09215d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------------------------------------------------------------+----------------------------------------------------+\n",
            "|id |sentence                                                    |filtered_sentence                                   |\n",
            "+---+------------------------------------------------------------+----------------------------------------------------+\n",
            "|1  |[Spark, is, an, open-source, distributed, computing, system]|[Spark, open-source, distributed, computing, system]|\n",
            "|2  |[IT, has, interfaces, for, multiple, languages]             |[interfaces, multiple, languages]                   |\n",
            "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |[wide, range, libraries, APIs]                      |\n",
            "+---+------------------------------------------------------------+----------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# display the dataframe\n",
        "textData.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoqXWjM2EJhb"
      },
      "outputs": [],
      "source": [
        "#import StringIndexer\n",
        "from pyspark.ml.feature import StringIndexer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-bDLP5aEJhc"
      },
      "outputs": [],
      "source": [
        "#create a dataframe with sample text and display it\n",
        "colors = spark.createDataFrame(\n",
        "    [(0, \"red\"), (1, \"red\"), (2, \"blue\"), (3, \"yellow\" ), (4, \"yellow\"), (5, \"yellow\")],\n",
        "    [\"id\", \"color\"])\n",
        "\n",
        "colors.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7IOtzluEJhc"
      },
      "outputs": [],
      "source": [
        "# index the strings in the column \"color\" and store their indexes in the column \"colorIndex\"\n",
        "indexer = StringIndexer(inputCol=\"color\", outputCol=\"colorIndex\")\n",
        "indexed = indexer.fit(colors).transform(colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78dHTbIWEJhc"
      },
      "outputs": [],
      "source": [
        "# display the dataframe\n",
        "indexed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBg224WZEJhd"
      },
      "outputs": [],
      "source": [
        "#import StandardScaler\n",
        "from pyspark.ml.feature import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs85VhMSEJhd"
      },
      "outputs": [],
      "source": [
        "# Create a sample dataframe and display it\n",
        "from pyspark.ml.linalg import Vectors\n",
        "data = [(1, Vectors.dense([70, 170, 17])),\n",
        "        (2, Vectors.dense([80, 165, 25])),\n",
        "        (3, Vectors.dense([65, 150, 135]))]\n",
        "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZlbJFvOEJhd"
      },
      "outputs": [],
      "source": [
        "# Define the StandardScaler transformer\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBkoZy5yEJhd"
      },
      "outputs": [],
      "source": [
        "# Fit the transformer to the dataset\n",
        "scalerModel = scaler.fit(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM7WVl__EJhe"
      },
      "outputs": [],
      "source": [
        "# Scale the data\n",
        "scaledData = scalerModel.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVn09WA7EJhe"
      },
      "outputs": [],
      "source": [
        "# Show the scaled data\n",
        "scaledData.show(truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rr_d5YoEJhe"
      },
      "source": [
        "Stop Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXpTjtCKEJhe"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbxAsb5iEJhn"
      },
      "source": [
        "<!--\n",
        "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
        "|-|-|-|-|\n",
        "|2023-05-14|0.1|Ramesh Sannareddy|Initial Version Created|\n",
        "-->\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "prev_pub_hash": "4c21ca6799b5df9de8931185ed7970dda50592fa98e40cb08896d9bc6a728d0a",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}