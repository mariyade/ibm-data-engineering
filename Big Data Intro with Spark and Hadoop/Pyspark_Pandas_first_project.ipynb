{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZYk-jpeGb3J"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "id": "3DSi80OSGc3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "p3zPgDf6GgMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DateType\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"COVID-19 Data Analysis\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "if 'spark' in locals() and isinstance(spark, SparkSession):\n",
        "    print(\"SparkSession is active and ready to use.\")\n",
        "else:\n",
        "    print(\"SparkSession is not active. Please create a SparkSession.\")"
      ],
      "metadata": {
        "id": "udl4JQDAGiwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vaccination_data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KpHDlIzdtR63BdTofl1mOg/owid-covid-latest.csv')"
      ],
      "metadata": {
        "id": "em-cN7NzGkrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Displaying the first 5 records of the vaccination data:\")\n",
        "columns_to_display = ['continent', 'total_cases', 'total_deaths', 'total_vaccinations', 'population']\n",
        "print(vaccination_data[columns_to_display].head())"
      ],
      "metadata": {
        "id": "QMmT5j-yGmze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"continent\", StringType(), True),\n",
        "    StructField(\"total_cases\", LongType(), True),\n",
        "    StructField(\"total_deaths\", LongType(), True),\n",
        "    StructField(\"total_vaccinations\", LongType(), True),\n",
        "    StructField(\"population\", LongType(), True)\n",
        "])\n",
        "vaccination_data['continent'] = vaccination_data['continent'].astype(str)\n",
        "vaccination_data['total_cases'] = vaccination_data['total_cases'].fillna(0).astype('int64')\n",
        "vaccination_data['total_deaths'] = vaccination_data['total_deaths'].fillna(0).astype('int64')\n",
        "vaccination_data['total_vaccinations'] = vaccination_data['total_vaccinations'].fillna(0).astype('int64')\n",
        "vaccination_data['population'] = vaccination_data['population'].fillna(0).astype('int64')\n",
        "\n",
        "spark_df = spark.createDataFrame(vaccination_data[schema.fieldNames()])\n",
        "spark_df.show()"
      ],
      "metadata": {
        "id": "UpRFVYfRGqA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Schema of the Spark DataFrame:\")\n",
        "spark_df.printSchema()"
      ],
      "metadata": {
        "id": "rLZaC7JTGt3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_display = ['continent', 'total_cases', 'total_deaths', 'total_vaccinations', 'population']\n",
        "spark_df.select(columns_to_display).show(5)"
      ],
      "metadata": {
        "id": "hbZYfeqPG4wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Displaying the 'continent' and 'total_cases' columns:\")\n",
        "spark_df.select('continent', 'total_cases').show(5)"
      ],
      "metadata": {
        "id": "JZFh-LUwG7Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Filtering records where 'total_cases' is greater than 1,000,000:\")\n",
        "spark_df.filter(spark_df['total_cases'] > 1000000).show(5)"
      ],
      "metadata": {
        "id": "v2RCn8K_G96D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark_df_with_percentage = spark_df.withColumn(\n",
        "    'death_percentage',\n",
        "    (spark_df['total_deaths'] / spark_df['population']) * 100\n",
        ")\n",
        "spark_df_with_percentage = spark_df_with_percentage.withColumn(\n",
        "    'death_percentage',\n",
        "    F.concat(\n",
        "        F.format_number(spark_df_with_percentage['death_percentage'], 2),\n",
        "        F.lit('%')\n",
        "    )\n",
        ")\n",
        "columns_to_display = ['total_deaths', 'population', 'death_percentage', 'continent', 'total_vaccinations', 'total_cases']\n",
        "spark_df_with_percentage.select(columns_to_display).show(5)"
      ],
      "metadata": {
        "id": "luev1NeuHBQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calculating the total deaths per continent:\")\n",
        "spark_df.groupby(['continent']).agg({\"total_deaths\": \"SUM\"}).show()"
      ],
      "metadata": {
        "id": "7cSnPN3ZHFgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import IntegerType\n",
        "def convert_total_deaths(total_deaths):\n",
        "    return total_deaths * 2\n",
        "spark.udf.register(\"convert_total_deaths\", convert_total_deaths, IntegerType())"
      ],
      "metadata": {
        "id": "X4VCVB1SHJJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"DROP VIEW IF EXISTS data_v\")\n",
        "spark_df.createTempView('data_v')\n",
        "spark.sql('SELECT continent, total_deaths, convert_total_deaths(total_deaths) as converted_total_deaths FROM data_v').show()"
      ],
      "metadata": {
        "id": "KejP7PFmHMET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('SELECT * FROM data_v').show()"
      ],
      "metadata": {
        "id": "lH38UxLkHRCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Displaying continent with total vaccinated more than 1 million:\")\n",
        "spark.sql(\"SELECT continent FROM data_v WHERE total_vaccinations > 1000000\").show()"
      ],
      "metadata": {
        "id": "fEAMcQNCHSqR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}